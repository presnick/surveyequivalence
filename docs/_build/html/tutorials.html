
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Tutorials &#8212; SurveyEquivalence 1.0 documentation</title>
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="_static/alabaster.css" type="text/css" />
    <script id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="API" href="api.html" />
    <link rel="prev" title="Getting Started" href="getting_started.html" />
   
  <link rel="stylesheet" href="_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <div class="section" id="tutorials">
<h1>Tutorials<a class="headerlink" href="#tutorials" title="Permalink to this headline">¶</a></h1>
<div class="section" id="synthetic-running-example">
<h2>Synthetic Running Example<a class="headerlink" href="#synthetic-running-example" title="Permalink to this headline">¶</a></h2>
<p>The synthetic running example, as described in the paper, is implemented in the file <cite>examples/paper_running_example.py</cite>.</p>
<p>We first load the data that we created in <a class="reference internal" href="#generating-data-for-running-example"><span class="std std-ref">Generating Data for the Running Example</span></a></p>
<div class="highlight-python3 notranslate"><div class="highlight"><pre><span></span><span class="n">path</span> <span class="o">=</span> <span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">ROOT_DIR</span><span class="si">}</span><span class="s1">/data/running_example&#39;</span>

<span class="c1"># read the reference rater labels from file</span>
<span class="n">W</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">path</span><span class="si">}</span><span class="s2">/ref_rater_labels.csv&quot;</span><span class="p">,</span> <span class="n">index_col</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="c1"># read the predictions from file</span>
<span class="k">def</span> <span class="nf">str2prediction_instance</span><span class="p">(</span><span class="n">s</span><span class="p">):</span>
    <span class="c1"># s will be in format &quot;Prediction: [0.9, 0.1]&quot; or &quot;Prediction: neg&quot;</span>
    <span class="n">suffix</span> <span class="o">=</span> <span class="n">s</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;: &quot;</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">suffix</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="s1">&#39;[&#39;</span><span class="p">:</span>
        <span class="n">pr_pos</span><span class="p">,</span> <span class="n">pr_neg</span> <span class="o">=</span> <span class="n">suffix</span><span class="p">[</span><span class="mi">1</span><span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;,&#39;</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">DiscreteDistributionPrediction</span><span class="p">([</span><span class="s1">&#39;pos&#39;</span><span class="p">,</span> <span class="s1">&#39;neg&#39;</span><span class="p">],</span> <span class="p">[</span><span class="nb">float</span><span class="p">(</span><span class="n">pr_pos</span><span class="p">),</span> <span class="nb">float</span><span class="p">(</span><span class="n">pr_neg</span><span class="p">)])</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">DiscretePrediction</span><span class="p">(</span><span class="n">suffix</span><span class="p">)</span>
<span class="n">classifier_predictions</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">path</span><span class="si">}</span><span class="s2">/predictions.csv&quot;</span><span class="p">,</span> <span class="n">index_col</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">applymap</span><span class="p">(</span><span class="n">str2prediction_instance</span><span class="p">)</span>

<span class="n">hard_classifiers</span> <span class="o">=</span> <span class="n">classifier_predictions</span><span class="o">.</span><span class="n">columns</span><span class="p">[:</span><span class="mi">1</span><span class="p">]</span> <span class="c1"># [&#39;mock hard classifier&#39;]</span>
<span class="n">soft_classifiers</span> <span class="o">=</span> <span class="n">classifier_predictions</span><span class="o">.</span><span class="n">columns</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span> <span class="c1"># [&#39;calibrated hard classifier&#39;, &#39;h_infinity: ideal classifier&#39;]</span>
</pre></div>
</div>
<p>Note that the .csv file has the synthetic mock classifier predictions as strings.
We have to turn them into appropriate instances of <a class="reference internal" href="api.html#surveyequivalence.combiners.Prediction" title="surveyequivalence.combiners.Prediction"><code class="xref py py-class docutils literal notranslate"><span class="pre">surveyequivalence.combiners.Prediction</span></code></a>.
You will have to do something similar to convert numeric predictions of a real classifier for a real dataset into
instances of an appropriate subclass of Prediction.</p>
<p>Also note that W (the reference rater's labels) and classifier_predictions are two dataframes that must have the same
index, with the corresponding rows providing information about the same items.</p>
<div class="section" id="run-the-analysis-pipeline-for-pluralityvote-plus-agreementscore">
<h3>Run the analysis pipeline for PluralityVote plus AgreementScore<a class="headerlink" href="#run-the-analysis-pipeline-for-pluralityvote-plus-agreementscore" title="Permalink to this headline">¶</a></h3>
<p>Next, we run the analysis pipeline and generate plots. We will do this three times in all, with three different
pairings of combiner function with scoring function. We start with a scoring function that expects discrete label
predictions (i.e., hard classifiers). That requires a combiner function that produces a single predicted label from
a set of other labels.</p>
<p>We make an instance of the class <a class="reference internal" href="api.html#surveyequivalence.equivalence.AnalysisPipeline" title="surveyequivalence.equivalence.AnalysisPipeline"><code class="xref py py-class docutils literal notranslate"><span class="pre">surveyequivalence.equivalence.AnalysisPipeline</span></code></a>. We pass:</p>
<blockquote>
<div><ul class="simple">
<li><p>the <cite>.dataset</cite> as the rating matrix W.</p></li>
<li><p>the list of all column names as the reference rater's column names</p></li>
<li><p>the plurality_combiner; with binary labels, it just selects the label that was more popular for this item</p></li>
<li><p>for the scorer, agreement_score returns the percentage of items for which the predicted label matches the actual.</p></li>
</ul>
</div></blockquote>
<div class="highlight-python3 notranslate"><div class="highlight"><pre><span></span><span class="n">plurality_combiner</span> <span class="o">=</span> <span class="n">PluralityVote</span><span class="p">(</span><span class="n">allowable_labels</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;pos&#39;</span><span class="p">,</span> <span class="s1">&#39;neg&#39;</span><span class="p">])</span>
<span class="n">agreement_score</span> <span class="o">=</span> <span class="n">AgreementScore</span><span class="p">()</span>
<span class="n">pipeline</span> <span class="o">=</span> <span class="n">AnalysisPipeline</span><span class="p">(</span><span class="n">ds2</span><span class="o">.</span><span class="n">dataset</span><span class="p">,</span>
                            <span class="n">expert_cols</span><span class="o">=</span><span class="nb">list</span><span class="p">(</span><span class="n">ds2</span><span class="o">.</span><span class="n">dataset</span><span class="o">.</span><span class="n">columns</span><span class="p">),</span>
                            <span class="n">classifier_predictions</span><span class="o">=</span><span class="n">ds2</span><span class="o">.</span><span class="n">classifier_predictions</span><span class="p">[</span><span class="n">hard_classifiers</span><span class="p">],</span>
                            <span class="n">combiner</span><span class="o">=</span><span class="n">plurality_combiner</span><span class="p">,</span>
                            <span class="n">scorer</span><span class="o">=</span><span class="n">agreement_score</span><span class="p">,</span>
                            <span class="n">allowable_labels</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;pos&#39;</span><span class="p">,</span> <span class="s1">&#39;neg&#39;</span><span class="p">],</span>
                            <span class="n">num_bootstrap_item_samples</span><span class="o">=</span><span class="n">num_bootstrap_item_samples</span><span class="p">,</span>
                            <span class="n">verbosity</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">pipeline</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">path</span> <span class="o">=</span> <span class="n">pipeline</span><span class="o">.</span><span class="n">path_for_saving</span><span class="p">(</span><span class="s2">&quot;running_example/plurality_plus_agreement&quot;</span><span class="p">),</span>
    <span class="n">msg</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;&quot;&quot;</span>
<span class="s2">Running example with </span><span class="si">{</span><span class="n">num_items_per_dataset</span><span class="si">}</span><span class="s2"> items and </span><span class="si">{</span><span class="n">num_labels_per_item</span><span class="si">}</span><span class="s2"> raters per item</span>
<span class="si">{</span><span class="n">num_bootstrap_item_samples</span><span class="si">}</span><span class="s2"> bootstrap itemsets</span>
<span class="s2">Plurality combiner with agreement score</span>
<span class="s2">&quot;&quot;&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="plot-the-results">
<h3>Plot the results<a class="headerlink" href="#plot-the-results" title="Permalink to this headline">¶</a></h3>
<p>Next we create an instance of the class <a class="reference internal" href="api.html#surveyequivalence.equivalence.Plot" title="surveyequivalence.equivalence.Plot"><code class="xref py py-class docutils literal notranslate"><span class="pre">surveyequivalence.equivalence.Plot</span></code></a>, passing:</p>
<blockquote>
<div><ul class="simple">
<li><p>the power curve that we calculated in the AnalysisPipeline</p></li>
<li><p>classifier scores calculated in the AnalysisPipeline</p></li>
<li><p>The color_map says what colors to use for the different components of the plot. In this case, we don't have
an amateur_power_curve, but we have included it to illustrate how to supply a color for it if we did have
that additional power curve for other raters.</p></li>
</ul>
</div></blockquote>
<div class="highlight-python3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">()</span>
<span class="n">fig</span><span class="o">.</span><span class="n">set_size_inches</span><span class="p">(</span><span class="mf">8.5</span><span class="p">,</span> <span class="mf">10.5</span><span class="p">)</span>

<span class="n">color_map</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;expert_power_curve&#39;</span><span class="p">:</span> <span class="s1">&#39;black&#39;</span><span class="p">,</span>
    <span class="s1">&#39;amateur_power_curve&#39;</span><span class="p">:</span> <span class="s1">&#39;green&#39;</span><span class="p">,</span>
    <span class="s1">&#39;hard classifier&#39;</span><span class="p">:</span> <span class="s1">&#39;red&#39;</span><span class="p">,</span>
    <span class="s1">&#39;mock classifier&#39;</span><span class="p">:</span> <span class="s1">&#39;blue&#39;</span><span class="p">,</span>
    <span class="s1">&#39;calibrated hard classifier&#39;</span><span class="p">:</span> <span class="s1">&#39;red&#39;</span>
<span class="p">}</span>

<span class="n">pl</span> <span class="o">=</span> <span class="n">Plot</span><span class="p">(</span><span class="n">ax</span><span class="p">,</span>
          <span class="n">pipeline</span><span class="o">.</span><span class="n">expert_power_curve</span><span class="p">,</span>
          <span class="n">classifier_scores</span><span class="o">=</span><span class="n">pipeline</span><span class="o">.</span><span class="n">classifier_scores</span><span class="p">,</span>
          <span class="n">color_map</span><span class="o">=</span><span class="n">color_map</span><span class="p">,</span>
          <span class="n">y_axis_label</span><span class="o">=</span><span class="s1">&#39;percent agreement with reference rater&#39;</span><span class="p">,</span>
          <span class="n">y_range</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
          <span class="n">name</span><span class="o">=</span><span class="s1">&#39;running example: majority vote + agreement score&#39;</span><span class="p">,</span>
          <span class="n">legend_label</span><span class="o">=</span><span class="s1">&#39;k raters&#39;</span><span class="p">,</span>
          <span class="n">generate_pgf</span><span class="o">=</span><span class="kc">True</span>
          <span class="p">)</span>
</pre></div>
</div>
<p>Then, we call the method <a class="reference internal" href="api.html#surveyequivalence.equivalence.Plot.plot" title="surveyequivalence.equivalence.Plot.plot"><code class="xref py py-meth docutils literal notranslate"><span class="pre">surveyequivalence.equivalence.Plot.plot()</span></code></a> to actually create the plot.</p>
<div class="highlight-python3 notranslate"><div class="highlight"><pre><span></span><span class="n">pl</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">include_classifiers</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">include_classifier_equivalences</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">include_droplines</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">include_expert_points</span><span class="o">=</span><span class="s1">&#39;all&#39;</span><span class="p">,</span>
        <span class="n">connect_expert_points</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">include_classifier_cis</span><span class="o">=</span><span class="kc">True</span>
        <span class="p">)</span>
</pre></div>
</div>
<p>Finally, we save the plot, using <a class="reference internal" href="api.html#surveyequivalence.equivalence.Plot.save" title="surveyequivalence.equivalence.Plot.save"><code class="xref py py-meth docutils literal notranslate"><span class="pre">surveyequivalence.equivalence.Plot.save()</span></code></a>. This saves both a PDF version and, since we specified that we wanted it,
a pgf file suitable for importing into latex.</p>
<div class="highlight-python3 notranslate"><div class="highlight"><pre><span></span><span class="n">pl</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">pipeline</span><span class="o">.</span><span class="n">path_for_saving</span><span class="p">(</span><span class="s2">&quot;running_example/plurality_plus_agreement&quot;</span><span class="p">),</span> <span class="n">fig</span><span class="o">=</span><span class="n">fig</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="anonymousbayesiancombiner-plus-crossentropy">
<h3>AnonymousBayesianCombiner plus CrossEntropy<a class="headerlink" href="#anonymousbayesiancombiner-plus-crossentropy" title="Permalink to this headline">¶</a></h3>
<p>Next we consider a scorer for soft classifiers, which predict a probability for each possible label, rather than
outputting a single label. The Anonymous Bayesian Combiner, as described in the paper, is one such combiner.
Essentially, it estimates the probability of a pos or neg next label conditional on having observed the labels
that have been seen so far.</p>
<p>The analysis code is similar to that for the previous combiner and scorer.</p>
<div class="highlight-python3 notranslate"><div class="highlight"><pre><span></span><span class="n">abc</span> <span class="o">=</span> <span class="n">AnonymousBayesianCombiner</span><span class="p">(</span><span class="n">allowable_labels</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;pos&#39;</span><span class="p">,</span> <span class="s1">&#39;neg&#39;</span><span class="p">])</span>
<span class="n">cross_entropy</span> <span class="o">=</span> <span class="n">CrossEntropyScore</span><span class="p">()</span>
<span class="n">pipeline2</span> <span class="o">=</span> <span class="n">AnalysisPipeline</span><span class="p">(</span><span class="n">ds2</span><span class="o">.</span><span class="n">dataset</span><span class="p">,</span>
                            <span class="n">expert_cols</span><span class="o">=</span><span class="nb">list</span><span class="p">(</span><span class="n">ds2</span><span class="o">.</span><span class="n">dataset</span><span class="o">.</span><span class="n">columns</span><span class="p">),</span>
                            <span class="n">classifier_predictions</span><span class="o">=</span><span class="n">ds2</span><span class="o">.</span><span class="n">classifier_predictions</span><span class="p">[</span><span class="n">soft_classifiers</span><span class="p">],</span>
                            <span class="n">combiner</span><span class="o">=</span><span class="n">abc</span><span class="p">,</span>
                            <span class="n">scorer</span><span class="o">=</span><span class="n">cross_entropy</span><span class="p">,</span>
                            <span class="n">allowable_labels</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;pos&#39;</span><span class="p">,</span> <span class="s1">&#39;neg&#39;</span><span class="p">],</span>
                            <span class="n">num_bootstrap_item_samples</span><span class="o">=</span><span class="n">num_bootstrap_item_samples</span><span class="p">,</span>
                            <span class="n">verbosity</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>

<span class="n">pipeline2</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">path</span><span class="o">=</span><span class="n">pipeline</span><span class="o">.</span><span class="n">path_for_saving</span><span class="p">(</span><span class="s2">&quot;running_example/abc_plus_cross_entropy&quot;</span><span class="p">),</span>
               <span class="n">msg</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;&quot;&quot;</span>
<span class="s2">Running example with </span><span class="si">{</span><span class="n">num_items_per_dataset</span><span class="si">}</span><span class="s2"> items and </span><span class="si">{</span><span class="n">num_labels_per_item</span><span class="si">}</span><span class="s2"> raters per item</span>
<span class="si">{</span><span class="n">num_bootstrap_item_samples</span><span class="si">}</span><span class="s2"> bootstrap itemsets</span>
<span class="s2">Anonymous Bayesian combiner with cross entropy score</span>
<span class="s2">&quot;&quot;&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>The plotting is similar, with a couple twists.</p>
<p>Here we specify centering of y-axis values, subtracting out the score for a survey of k=0 people.
With the cross entropy scoring
function these centered values have a natural interpretation, as explained in the paper. The cross entropy of a
baseline classifier that predicts the overall empirical frequency of the labels (i.e., Anonymous Bayesian Combiner
with k=0) against
a reference rater's labels will approach the
entropy of the distribution from which reference raters are drawn, as the number of items grows. Thus,
the cross-entropy of any other classifier minus this score estimates the
information gain of the classifier (mutual information of the classifier with a random reference rater's predictions).</p>
<p>Note that we are choosing to plot only the calibrated hard classifier, and not the ideal classifier. In the pipeline
we calculated results for two soft classifiers. Because here we choose
to plot a horizontal line for only one of those two classifiers, we need to make a new instance of ClassifierResults
passing in only that column from the dataframe in the <cite>.classifier_scores</cite> object.</p>
<p>You may find it instructive to change the code to <code class="code docutils literal notranslate"><span class="pre">classifier_scores=pipeline2.classifier_scores</span></code>, and notice that the
resulting graph adds an extra horizontal line for the ideal classifier.</p>
<div class="highlight-python3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">()</span>
<span class="n">fig</span><span class="o">.</span><span class="n">set_size_inches</span><span class="p">(</span><span class="mf">8.5</span><span class="p">,</span> <span class="mf">10.5</span><span class="p">)</span>

<span class="n">pl</span> <span class="o">=</span> <span class="n">Plot</span><span class="p">(</span><span class="n">ax</span><span class="p">,</span>
          <span class="n">pipeline2</span><span class="o">.</span><span class="n">expert_power_curve</span><span class="p">,</span>
          <span class="n">classifier_scores</span><span class="o">=</span><span class="n">ClassifierResults</span><span class="p">(</span><span class="n">pipeline2</span><span class="o">.</span><span class="n">classifier_scores</span><span class="o">.</span><span class="n">df</span><span class="p">[[</span><span class="s1">&#39;calibrated hard classifier&#39;</span><span class="p">]]),</span>
          <span class="n">color_map</span><span class="o">=</span><span class="n">color_map</span><span class="p">,</span>
          <span class="n">y_axis_label</span><span class="o">=</span><span class="s1">&#39;information gain ($c_k - c_0$)&#39;</span><span class="p">,</span>
          <span class="n">center_on</span><span class="o">=</span><span class="n">pipeline2</span><span class="o">.</span><span class="n">expert_power_curve</span><span class="o">.</span><span class="n">values</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
          <span class="n">y_range</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">),</span>
          <span class="n">name</span><span class="o">=</span><span class="s1">&#39;running example: ABC + cross entropy&#39;</span><span class="p">,</span>
          <span class="n">legend_label</span><span class="o">=</span><span class="s1">&#39;k raters&#39;</span><span class="p">,</span>
          <span class="n">generate_pgf</span><span class="o">=</span><span class="kc">True</span>
          <span class="p">)</span>

<span class="n">pl</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">include_classifiers</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">include_classifier_equivalences</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">include_droplines</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">include_expert_points</span><span class="o">=</span><span class="s1">&#39;all&#39;</span><span class="p">,</span>
        <span class="n">connect_expert_points</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">include_classifier_cis</span><span class="o">=</span><span class="kc">True</span> <span class="c1">##change back to false</span>
        <span class="p">)</span>
<span class="n">pl</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">path</span><span class="o">=</span><span class="n">pipeline</span><span class="o">.</span><span class="n">path_for_saving</span><span class="p">(</span><span class="s2">&quot;running_example/abc_plus_cross_entropy&quot;</span><span class="p">),</span> <span class="n">fig</span><span class="o">=</span><span class="n">fig</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="frequencycombiner-plus-crossentropy">
<h3>FrequencyCombiner plus CrossEntropy<a class="headerlink" href="#frequencycombiner-plus-crossentropy" title="Permalink to this headline">¶</a></h3>
<p>The code for the last combiner and scorer is very similar and is omitted.</p>
</div>
<div class="section" id="where-to-find-the-results">
<h3>Where to Find the Results<a class="headerlink" href="#where-to-find-the-results" title="Permalink to this headline">¶</a></h3>
<p>In config.py, you will specify a ROOTDIR.</p>
<p>Directory f'{ROOT_DIR}/saved_analyses' will have a folder named with a timestamp for the start of your AnalysisPipeline
run. Look inside that to find three subdirectories, one for each combiner+scorer pairing.</p>
</div>
</div>
<div class="section" id="generating-data-for-the-running-example">
<span id="generating-data-for-running-example"></span><h2>Generating Data for the Running Example<a class="headerlink" href="#generating-data-for-the-running-example" title="Permalink to this headline">¶</a></h2>
<p>The dataset use in the running example is synthetic. We generated it using the function <a class="reference internal" href="api.html#surveyequivalence.synthetic_datasets.make_running_example_dataset" title="surveyequivalence.synthetic_datasets.make_running_example_dataset"><code class="xref py py-func docutils literal notranslate"><span class="pre">surveyequivalence.synthetic_datasets.make_running_example_dataset()</span></code></a>.</p>
<div class="highlight-python3 notranslate"><div class="highlight"><pre><span></span><span class="n">num_items_per_dataset</span><span class="o">=</span><span class="mi">1000</span>
<span class="n">num_labels_per_item</span><span class="o">=</span><span class="mi">10</span>
<span class="n">num_bootstrap_item_samples</span> <span class="o">=</span> <span class="mi">500</span>

<span class="n">ds</span> <span class="o">=</span> <span class="n">make_running_example_dataset</span><span class="p">(</span><span class="n">minimal</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">num_items_per_dataset</span><span class="o">=</span><span class="n">num_items_per_dataset</span><span class="p">,</span>
                                   <span class="n">num_labels_per_item</span><span class="o">=</span><span class="n">num_labels_per_item</span><span class="p">,</span>
                                   <span class="n">include_soft_classifier</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">include_hard_classifier</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="n">ds</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">dirname</span><span class="o">=</span><span class="s1">&#39;running_example&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p>The resulting SyntheticDataset object has an attribute <cite>.classifier_predictions</cite>, which is a dataframe with one column
each for several classifiers.</p>
<blockquote>
<div><ul class="simple">
<li><p>'mock hard classifier': a mock classifier that outputs 90/10 pos labels for high state, 50/50 for med,
and 05/95 for low. This classifier is more informative than a single reference rater,
whose labels are generated 80/20, 50/50, and 10/90.</p></li>
<li><p>'calibrated hard classifier': a mock classifier that converts the hard classifier outputs to their correct
calibrated soft predictions (probability that the next reference rater will have a positive label).</p></li>
<li><p>'h_infinity: ideal classifier': a mock classifier that correctly predicts 80/20, 50/50 or 10/90 for every item,
magically knowing the item's true state. No classifier can achieve higher (expected)
cross-entropy score than this classifier.</p></li>
</ul>
</div></blockquote>
<p>Two .csv files are generated, predictions.csv and ref_rater_labels.csv. They are stored in a subdirectory of
data/running_example.</p>
</div>
<div class="section" id="jigsaw-toxicity-dataset-analysis">
<h2>Jigsaw Toxicity Dataset Analysis<a class="headerlink" href="#jigsaw-toxicity-dataset-analysis" title="Permalink to this headline">¶</a></h2>
<p>Calculating the survey equivalence of an real world item and rater set is easy with this package. Here we focus on the
Jigsaw Toxcitiy Dataset. This dataset is originally discussed in this paper:</p>
<p>Wulczyn, E., Thain, N., &amp; Dixon, L. (2017, April). Ex machina: Personal attacks seen at scale. In Proceedings of the
26th international conference on world wide web (pp. 1391-1399).</p>
<p>The dataset can be found in <cite>data/wiki_attack_labels_and_predictor.csv</cite>. It contains raters labels of whether or not
some comment on Wikipedia is a personal attack. The header and first row of the dataset are:</p>
<p><cite>rev_id,perc_labelled_attack,n_labelled_attack,n_labels,predictor_prob</cite>
<cite>155243,0.222222222,2,9,0.037257579</cite></p>
<p>where the columns represent the Wikipedia comment ID, the percentage of labels that indicated the the comment was an
attack, the number of labeled attacks, the number of total labels -- where the percentage is equal to the number of
attacks divided by the number of labels, and the probability that the Jigsaw predictor returned.</p>
<p>We load and perform surveyequivalence analysis in <cite>examples/toxicity.py</cite></p>
<div class="section" id="example-driver">
<h3>Example Driver<a class="headerlink" href="#example-driver" title="Permalink to this headline">¶</a></h3>
<p>The main function servers as a driver for four combinations of scoring and combiner functions. AnonymousBayesianCombiner
with CrossEntropy, which has several desirable properties as discussed in the paper. Combinations of FrequencyCombiner
and AUCScore are also performed.</p>
<p>The first four lines of the main method are very important for the execution of the analysis. The <cite>max_k</cite> parameter
limits the number of raters to consider. The <cite>max_items</cite> parameter truncates the dataset -- large datasets take a long
time to run; full experiments must carefully weight limiting the dataset. The <cite>bootstrap_samples</cite> parameter indicates
how many times to sample the surveyequivelance to generate confidence intervals. The <cite>num_processors</cite> indicates how many
processors to use for computing.</p>
<p>In the reported experimental results, we set <cite>max_k</cite> = 10, <cite>max_items</cite> = 2000, <cite>bootstrap_samples</cite> = 500, and we had a
20 core compute server available to us. With these parameters, the full dataset took about 12 hours to compute each
combiner/score pair (two days for the whole driver to complete).</p>
<div class="highlight-python3 notranslate"><div class="highlight"><pre><span></span><span class="n">max_k</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">max_items</span> <span class="o">=</span> <span class="mi">20</span>
<span class="n">bootstrap_samples</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">num_processors</span> <span class="o">=</span> <span class="mi">3</span>

<span class="c1"># Next we iterate over various combinations of combiner and scoring functions.</span>
<span class="n">combiner</span> <span class="o">=</span> <span class="n">AnonymousBayesianCombiner</span><span class="p">(</span><span class="n">allowable_labels</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;a&#39;</span><span class="p">,</span> <span class="s1">&#39;n&#39;</span><span class="p">])</span>
<span class="n">scorer</span> <span class="o">=</span> <span class="n">CrossEntropyScore</span><span class="p">()</span>
<span class="n">run</span><span class="p">(</span><span class="n">combiner</span><span class="o">=</span><span class="n">combiner</span><span class="p">,</span> <span class="n">scorer</span><span class="o">=</span><span class="n">scorer</span><span class="p">,</span> <span class="n">max_k</span><span class="o">=</span><span class="n">max_k</span><span class="p">,</span> <span class="n">max_items</span><span class="o">=</span><span class="n">max_items</span><span class="p">,</span> <span class="n">bootstrap_samples</span><span class="o">=</span><span class="n">bootstrap_samples</span><span class="p">,</span>
    <span class="n">num_processors</span><span class="o">=</span><span class="n">num_processors</span><span class="p">)</span>

<span class="n">combiner</span> <span class="o">=</span> <span class="n">FrequencyCombiner</span><span class="p">(</span><span class="n">allowable_labels</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;a&#39;</span><span class="p">,</span> <span class="s1">&#39;n&#39;</span><span class="p">])</span>
<span class="n">scorer</span> <span class="o">=</span> <span class="n">CrossEntropyScore</span><span class="p">()</span>
<span class="n">run</span><span class="p">(</span><span class="n">combiner</span><span class="o">=</span><span class="n">combiner</span><span class="p">,</span> <span class="n">scorer</span><span class="o">=</span><span class="n">scorer</span><span class="p">,</span> <span class="n">max_k</span><span class="o">=</span><span class="n">max_k</span><span class="p">,</span> <span class="n">max_items</span><span class="o">=</span><span class="n">max_items</span><span class="p">,</span> <span class="n">bootstrap_samples</span><span class="o">=</span><span class="n">bootstrap_samples</span><span class="p">,</span>
    <span class="n">num_processors</span><span class="o">=</span><span class="n">num_processors</span><span class="p">)</span>

<span class="n">combiner</span> <span class="o">=</span> <span class="n">AnonymousBayesianCombiner</span><span class="p">(</span><span class="n">allowable_labels</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;a&#39;</span><span class="p">,</span> <span class="s1">&#39;n&#39;</span><span class="p">])</span>
<span class="n">scorer</span> <span class="o">=</span> <span class="n">AUCScore</span><span class="p">()</span>
<span class="n">run</span><span class="p">(</span><span class="n">combiner</span><span class="o">=</span><span class="n">combiner</span><span class="p">,</span> <span class="n">scorer</span><span class="o">=</span><span class="n">scorer</span><span class="p">,</span> <span class="n">max_k</span><span class="o">=</span><span class="n">max_k</span><span class="p">,</span> <span class="n">max_items</span><span class="o">=</span><span class="n">max_items</span><span class="p">,</span> <span class="n">bootstrap_samples</span><span class="o">=</span><span class="n">bootstrap_samples</span><span class="p">,</span>
    <span class="n">num_processors</span><span class="o">=</span><span class="n">num_processors</span><span class="p">)</span>

<span class="n">combiner</span> <span class="o">=</span> <span class="n">FrequencyCombiner</span><span class="p">(</span><span class="n">allowable_labels</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;a&#39;</span><span class="p">,</span> <span class="s1">&#39;n&#39;</span><span class="p">])</span>
<span class="n">scorer</span> <span class="o">=</span> <span class="n">AUCScore</span><span class="p">()</span>
<span class="n">run</span><span class="p">(</span><span class="n">combiner</span><span class="o">=</span><span class="n">combiner</span><span class="p">,</span> <span class="n">scorer</span><span class="o">=</span><span class="n">scorer</span><span class="p">,</span> <span class="n">max_k</span><span class="o">=</span><span class="n">max_k</span><span class="p">,</span> <span class="n">max_items</span><span class="o">=</span><span class="n">max_items</span><span class="p">,</span> <span class="n">bootstrap_samples</span><span class="o">=</span><span class="n">bootstrap_samples</span><span class="p">,</span>
    <span class="n">num_processors</span><span class="o">=</span><span class="n">num_processors</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="loading-the-dataset">
<h3>Loading the Dataset<a class="headerlink" href="#loading-the-dataset" title="Permalink to this headline">¶</a></h3>
<p>The first step is to load the dataset. Importantly, the surveyequivalence functions assume that the data exists in a
maxtrix form with n rows for each item (Wiki-comment in this case), and m columns for each rater. However, the dataset,
as exists, only provides counts. So it is important that we reverse-engineer each item and estimate what each rater
might have done. This is ok, because the rater ids (i.e, individual columns) are not important -- although this might
be something for future work.</p>
<div class="highlight-python3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Load the dataset as a pandas dataframe</span>
<span class="n">wiki</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">ROOT_DIR</span><span class="si">}</span><span class="s1">/data/wiki_attack_labels_and_predictor.csv&#39;</span><span class="p">)</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>

<span class="c1"># X and Y for calibration. These lists are matched</span>
<span class="n">X</span> <span class="o">=</span> <span class="nb">list</span><span class="p">()</span>
<span class="n">y</span> <span class="o">=</span> <span class="nb">list</span><span class="p">()</span>

<span class="c1"># Create rating pairs from the dataset</span>
<span class="k">for</span> <span class="n">index</span><span class="p">,</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">wiki</span><span class="o">.</span><span class="n">iterrows</span><span class="p">():</span>

    <span class="n">raters</span> <span class="o">=</span> <span class="nb">list</span><span class="p">()</span>

    <span class="n">n_raters</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">item</span><span class="p">[</span><span class="s1">&#39;n_labels&#39;</span><span class="p">])</span>
    <span class="n">n_labelled_attack</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">item</span><span class="p">[</span><span class="s1">&#39;n_labelled_attack&#39;</span><span class="p">])</span>

    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_labelled_attack</span><span class="p">):</span>
        <span class="n">raters</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s1">&#39;a&#39;</span><span class="p">)</span>
        <span class="n">X</span><span class="o">.</span><span class="n">append</span><span class="p">([</span><span class="n">item</span><span class="p">[</span><span class="s1">&#39;predictor_prob&#39;</span><span class="p">],</span> <span class="n">n_raters</span><span class="p">])</span>
        <span class="n">y</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_raters</span> <span class="o">-</span> <span class="n">n_labelled_attack</span><span class="p">):</span>
        <span class="n">raters</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s1">&#39;n&#39;</span><span class="p">)</span>
        <span class="n">X</span><span class="o">.</span><span class="n">append</span><span class="p">([</span><span class="n">item</span><span class="p">[</span><span class="s1">&#39;predictor_prob&#39;</span><span class="p">],</span> <span class="n">n_raters</span><span class="p">])</span>
        <span class="n">y</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

    <span class="n">shuffle</span><span class="p">(</span><span class="n">raters</span><span class="p">)</span>

    <span class="c1"># This is the predictor i.e., score for toxic comment. It will be at index 0 in W.</span>
    <span class="n">dataset</span><span class="p">[</span><span class="n">index</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="n">item</span><span class="p">[</span><span class="s1">&#39;predictor_prob&#39;</span><span class="p">]]</span> <span class="o">+</span> <span class="n">raters</span>
</pre></div>
</div>
<p>At this point the <cite>dataset</cite> variable will have one row for each item (i.e., Wiki-comment) and a shuffled listing of 'a'
and 'n' indicating attack or not-attack.</p>
<p>This dataset is not yet in matrix form. We need to convert what is essentially an adjacency list into an adjacency
matrix. To do this we find the max number of raters and set the number of columns to that number and pad the dataset
with Nones for items with less than the max number of raters.</p>
<div class="highlight-python3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Determine the number of columns needed in W. This is the max number of raters for an item.</span>
<span class="n">length</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="nb">map</span><span class="p">(</span><span class="nb">len</span><span class="p">,</span> <span class="n">dataset</span><span class="o">.</span><span class="n">values</span><span class="p">()))</span>

<span class="c1"># Pad W with Nones if the number of raters for some item is less than the max.</span>
<span class="n">padded_dataset</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">xi</span> <span class="o">+</span> <span class="p">[</span><span class="kc">None</span><span class="p">]</span> <span class="o">*</span> <span class="p">(</span><span class="n">length</span> <span class="o">-</span> <span class="nb">len</span><span class="p">(</span><span class="n">xi</span><span class="p">))</span> <span class="k">for</span> <span class="n">xi</span> <span class="ow">in</span> <span class="n">dataset</span><span class="o">.</span><span class="n">values</span><span class="p">()])</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;##Wiki Toxic - Dataset loaded##&#39;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">padded_dataset</span><span class="p">))</span>

<span class="c1"># Trim the dataset to only the first max_items and recast W as a dataframe</span>
<span class="n">W</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">padded_dataset</span><span class="p">)[:</span><span class="n">max_items</span><span class="p">]</span>

<span class="c1"># Recall that index 0 was the classifier output, i.e., toxicity score. We relabel this to &#39;soft classifier&#39; to keep</span>
<span class="c1"># track of it.</span>
<span class="n">W</span> <span class="o">=</span> <span class="n">W</span><span class="o">.</span><span class="n">rename</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">{</span><span class="mi">0</span><span class="p">:</span> <span class="s1">&#39;soft classifier&#39;</span><span class="p">})</span>
</pre></div>
</div>
<p>Here <cite>W</cite> is the item-rater matrix. We trim it to <cite>max_items</cite> to reduce the size of the dataset. There are very many
items, and it would be difficult to consider all of them in a tutorial.</p>
</div>
<div class="section" id="calibrating-the-predictor">
<h3>Calibrating the Predictor<a class="headerlink" href="#calibrating-the-predictor" title="Permalink to this headline">¶</a></h3>
<p>Next we are concerned with calibrating our classifier.</p>
<p>The Wiki-toxicity predictor was labeled <cite>predictor_prob</cite> in the dataset, and was loaded, for each rater, into <cite>X</cite>,
which is associated with a 1 or a 0 in <cite>y</cite> if the rater labeled attack or not attack respectively. The goal of this
predictor is to not necessarily predict attack or not attack, but rather to give a probability of the label. This
probability is a kind of confidence about the prediction.</p>
<p>Calibrating the predictor provides a way for the <cite>predictor_prob</cite> to be directly interprid as a confidence level. That
is, if <cite>predictor_prob</cite> is well calibrated then for Wiki-comments it gave an attack value of 0.2, then about 20% of the
items it labelled as attack are actually attacks.</p>
<p>We use sklearns CalibratedClassifierCV class and the isotonic regressor to fit a calibrator.</p>
<div class="highlight-python3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Calculate calibration probabilities. Use the current hour as random seed, because these lists need to be matched</span>
<span class="n">calibrator</span> <span class="o">=</span> <span class="n">CalibratedClassifierCV</span><span class="p">(</span><span class="n">LinearSVC</span><span class="p">(</span><span class="n">max_iter</span><span class="o">=</span><span class="mi">1000</span><span class="p">),</span> <span class="n">method</span><span class="o">=</span><span class="s1">&#39;isotonic&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">([</span><span class="n">x</span> <span class="k">for</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">X</span><span class="p">]),</span> <span class="n">y</span><span class="p">,</span>
                                                                   <span class="n">sample_weight</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span> <span class="o">/</span> <span class="n">y</span> <span class="k">for</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">X</span><span class="p">])</span>
</pre></div>
</div>
<p>Then we create our classifiers (predictors technically). For each item in W, we create a DiscreteDistributionPrediction
with attack and not-attack labels and 'a' and 'n' respectively. These are associated with the uncalibrated and
calibrated normalized Jigsaw predictor-probabilities.</p>
<div class="highlight-python3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Let&#39;s keep one classifier uncalibrated</span>
<span class="n">uncalibrated_classifier</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span>
    <span class="p">[</span><span class="n">DiscreteDistributionPrediction</span><span class="p">([</span><span class="s1">&#39;a&#39;</span><span class="p">,</span> <span class="s1">&#39;n&#39;</span><span class="p">],</span> <span class="p">[</span><span class="n">attack_prob</span><span class="p">,</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">attack_prob</span><span class="p">],</span> <span class="n">normalize</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
     <span class="k">for</span> <span class="n">attack_prob</span>
     <span class="ow">in</span> <span class="n">W</span><span class="p">[</span><span class="s1">&#39;soft classifier&#39;</span><span class="p">]],</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;Uncalibrated Jigsaw Toxicity Classifier&#39;</span><span class="p">])</span>

<span class="c1"># Create a calibrated classifier</span>
<span class="n">calibrated_classifier1</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span>
    <span class="p">[</span><span class="n">DiscreteDistributionPrediction</span><span class="p">([</span><span class="s1">&#39;a&#39;</span><span class="p">,</span> <span class="s1">&#39;n&#39;</span><span class="p">],</span> <span class="p">[</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">],</span> <span class="n">normalize</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
     <span class="k">for</span> <span class="n">b</span><span class="p">,</span> <span class="n">a</span>
     <span class="ow">in</span> <span class="n">calibrator</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">W</span><span class="o">.</span><span class="n">loc</span><span class="p">[:,</span> <span class="n">W</span><span class="o">.</span><span class="n">columns</span> <span class="o">==</span> <span class="s1">&#39;soft classifier&#39;</span><span class="p">])</span>
     <span class="p">],</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;Calibrated Jigsaw Toxicity Classifier&#39;</span><span class="p">])</span>

<span class="c1"># The classifier object now holds the classifier predictions. Let&#39;s remove this data from W now.</span>
<span class="n">W</span> <span class="o">=</span> <span class="n">W</span><span class="o">.</span><span class="n">drop</span><span class="p">([</span><span class="s1">&#39;soft classifier&#39;</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="n">classifiers</span> <span class="o">=</span> <span class="n">uncalibrated_classifier</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">calibrated_classifier1</span><span class="p">,</span> <span class="n">lsuffix</span><span class="o">=</span><span class="s1">&#39;left&#39;</span><span class="p">,</span> <span class="n">rsuffix</span><span class="o">=</span><span class="s1">&#39;right&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p>The last line concatenates these classifiers together into a single object so they can be passed together into the
plot function later on.</p>
</div>
<div class="section" id="calculating-a-prior-score">
<h3>Calculating a Prior Score<a class="headerlink" href="#calculating-a-prior-score" title="Permalink to this headline">¶</a></h3>
<p>In certain cases, like with the CrossEntropyScore, we don't care about the raw values, but rather about the
information gain that is provided by more and more raters. To measure the gain we first need to create a baseline
(i.e., prior) score from which we can (hopefully) improve.</p>
<div class="highlight-python3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Here we create a prior score. This is the c_0, i.e., the baseline score from which we measure information gain</span>
<span class="c1"># Information gain is only defined from cross entropy, so we only calculate this if the scorer is CrossEntropyScore</span>
<span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="n">scorer</span><span class="p">)</span> <span class="ow">is</span> <span class="n">CrossEntropyScore</span><span class="p">:</span>
    <span class="c1"># For the prior, we don&#39;t need any bootstrap samples and K needs to be only 1. Any improvement will be from k=2</span>
    <span class="c1"># k=3, etc.</span>
    <span class="n">prior</span> <span class="o">=</span> <span class="n">AnalysisPipeline</span><span class="p">(</span><span class="n">W</span><span class="p">,</span> <span class="n">combiner</span><span class="o">=</span><span class="n">AnonymousBayesianCombiner</span><span class="p">(</span><span class="n">allowable_labels</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;a&#39;</span><span class="p">,</span> <span class="s1">&#39;n&#39;</span><span class="p">]),</span> <span class="n">scorer</span><span class="o">=</span><span class="n">scorer</span><span class="p">,</span>
                             <span class="n">allowable_labels</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;a&#39;</span><span class="p">,</span> <span class="s1">&#39;n&#39;</span><span class="p">],</span> <span class="n">num_bootstrap_item_samples</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">verbosity</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                             <span class="n">classifier_predictions</span><span class="o">=</span><span class="n">classifiers</span><span class="p">,</span> <span class="n">max_K</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">procs</span><span class="o">=</span><span class="n">num_processors</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
    <span class="n">prior</span> <span class="o">=</span> <span class="kc">None</span>
</pre></div>
</div>
</div>
<div class="section" id="the-analysispipeline">
<h3>The AnalysisPipeline<a class="headerlink" href="#the-analysispipeline" title="Permalink to this headline">¶</a></h3>
<p>Now that we have the calibrated and uncalibrated predictors, the prior (if needed), and our item-rater dataset matrix
<cite>W</cite>, we can begin the surveyequivalence analysis</p>
<p>The AnalysisPipline takes in <cite>W</cite>, the combiner, scorer, and labels, and classifiers. The number of bootstrap samples
indicates how many tests to perform to create confidence intervals around the survey power curve. <cite>Max_k</cite> indicates
how large to grow the power curve, i.e., how many raters to consider in the limit. The AnalysisPipline does run in
parallel, so you can set the number of CPU cores to use with the <cite>num_processors</cite> parameter.</p>
<p>From here the plotting is very similar to the SyntheticRunningExample.</p>
</div>
</div>
<div class="section" id="guess-the-karma-dataset-analysis">
<h2>Guess the Karma Dataset Analysis<a class="headerlink" href="#guess-the-karma-dataset-analysis" title="Permalink to this headline">¶</a></h2>
</div>
<div class="section" id="credbank-dataset-analysis">
<h2>CredBank Dataset Analysis<a class="headerlink" href="#credbank-dataset-analysis" title="Permalink to this headline">¶</a></h2>
</div>
</div>


          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
<h1 class="logo"><a href="index.html">SurveyEquivalence</a></h1>








<h3>Navigation</h3>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="getting_started.html">Getting Started</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Tutorials</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#synthetic-running-example">Synthetic Running Example</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#run-the-analysis-pipeline-for-pluralityvote-plus-agreementscore">Run the analysis pipeline for PluralityVote plus AgreementScore</a></li>
<li class="toctree-l3"><a class="reference internal" href="#plot-the-results">Plot the results</a></li>
<li class="toctree-l3"><a class="reference internal" href="#anonymousbayesiancombiner-plus-crossentropy">AnonymousBayesianCombiner plus CrossEntropy</a></li>
<li class="toctree-l3"><a class="reference internal" href="#frequencycombiner-plus-crossentropy">FrequencyCombiner plus CrossEntropy</a></li>
<li class="toctree-l3"><a class="reference internal" href="#where-to-find-the-results">Where to Find the Results</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#generating-data-for-the-running-example">Generating Data for the Running Example</a></li>
<li class="toctree-l2"><a class="reference internal" href="#jigsaw-toxicity-dataset-analysis">Jigsaw Toxicity Dataset Analysis</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#example-driver">Example Driver</a></li>
<li class="toctree-l3"><a class="reference internal" href="#loading-the-dataset">Loading the Dataset</a></li>
<li class="toctree-l3"><a class="reference internal" href="#calibrating-the-predictor">Calibrating the Predictor</a></li>
<li class="toctree-l3"><a class="reference internal" href="#calculating-a-prior-score">Calculating a Prior Score</a></li>
<li class="toctree-l3"><a class="reference internal" href="#the-analysispipeline">The AnalysisPipeline</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#guess-the-karma-dataset-analysis">Guess the Karma Dataset Analysis</a></li>
<li class="toctree-l2"><a class="reference internal" href="#credbank-dataset-analysis">CredBank Dataset Analysis</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="api.html">API</a></li>
</ul>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="index.html">Documentation overview</a><ul>
      <li>Previous: <a href="getting_started.html" title="previous chapter">Getting Started</a></li>
      <li>Next: <a href="api.html" title="next chapter">API</a></li>
  </ul></li>
</ul>
</div>
<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" />
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script>$('#searchbox').show(0);</script>








        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;2021, Paul Resnick, Yuqing Kong, Grant Schoenebeck, Tim Weninger.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 3.5.0</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.12</a>
      
      |
      <a href="_sources/tutorials.rst.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>